{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from urllib.request import urlopen \n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext \n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"app1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = pd.read_csv(\"/Users/shubhambhardwaj/Desktop/Data_Science/Techgig/bank_fraud/application_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for cverting data frame (pandas) to spark dataframe\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "train_data = spark.createDataFrame(train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for directly reading a csv\n",
    "train_data = spark.read.option(\"header\",\"true\").option(\"inferschema\", \"true\").csv(\"/Users/shubhambhardwaj/Desktop/Data_Science/Techgig/bank_fraud/application_train.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#train_data.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SK_ID_CURR: integer (nullable = true)\n",
      " |-- TARGET: integer (nullable = true)\n",
      " |-- NAME_CONTRACT_TYPE: string (nullable = true)\n",
      " |-- CODE_GENDER: string (nullable = true)\n",
      " |-- FLAG_OWN_CAR: string (nullable = true)\n",
      " |-- FLAG_OWN_REALTY: string (nullable = true)\n",
      " |-- CNT_CHILDREN: integer (nullable = true)\n",
      " |-- AMT_INCOME_TOTAL: double (nullable = true)\n",
      " |-- AMT_CREDIT: double (nullable = true)\n",
      " |-- AMT_ANNUITY: double (nullable = true)\n",
      " |-- AMT_GOODS_PRICE: double (nullable = true)\n",
      " |-- NAME_TYPE_SUITE: string (nullable = true)\n",
      " |-- NAME_INCOME_TYPE: string (nullable = true)\n",
      " |-- NAME_EDUCATION_TYPE: string (nullable = true)\n",
      " |-- NAME_FAMILY_STATUS: string (nullable = true)\n",
      " |-- NAME_HOUSING_TYPE: string (nullable = true)\n",
      " |-- REGION_POPULATION_RELATIVE: double (nullable = true)\n",
      " |-- DAYS_BIRTH: integer (nullable = true)\n",
      " |-- DAYS_EMPLOYED: integer (nullable = true)\n",
      " |-- DAYS_REGISTRATION: double (nullable = true)\n",
      " |-- DAYS_ID_PUBLISH: integer (nullable = true)\n",
      " |-- OWN_CAR_AGE: integer (nullable = true)\n",
      " |-- FLAG_MOBIL: integer (nullable = true)\n",
      " |-- FLAG_EMP_PHONE: integer (nullable = true)\n",
      " |-- FLAG_WORK_PHONE: integer (nullable = true)\n",
      " |-- FLAG_CONT_MOBILE: integer (nullable = true)\n",
      " |-- FLAG_PHONE: integer (nullable = true)\n",
      " |-- FLAG_EMAIL: integer (nullable = true)\n",
      " |-- OCCUPATION_TYPE: string (nullable = true)\n",
      " |-- CNT_FAM_MEMBERS: integer (nullable = true)\n",
      " |-- REGION_RATING_CLIENT: integer (nullable = true)\n",
      " |-- REGION_RATING_CLIENT_W_CITY: integer (nullable = true)\n",
      " |-- WEEKDAY_APPR_PROCESS_START: string (nullable = true)\n",
      " |-- HOUR_APPR_PROCESS_START: integer (nullable = true)\n",
      " |-- REG_REGION_NOT_LIVE_REGION: integer (nullable = true)\n",
      " |-- REG_REGION_NOT_WORK_REGION: integer (nullable = true)\n",
      " |-- LIVE_REGION_NOT_WORK_REGION: integer (nullable = true)\n",
      " |-- REG_CITY_NOT_LIVE_CITY: integer (nullable = true)\n",
      " |-- REG_CITY_NOT_WORK_CITY: integer (nullable = true)\n",
      " |-- LIVE_CITY_NOT_WORK_CITY: integer (nullable = true)\n",
      " |-- ORGANIZATION_TYPE: string (nullable = true)\n",
      " |-- EXT_SOURCE_1: double (nullable = true)\n",
      " |-- EXT_SOURCE_2: double (nullable = true)\n",
      " |-- EXT_SOURCE_3: double (nullable = true)\n",
      " |-- APARTMENTS_AVG: double (nullable = true)\n",
      " |-- BASEMENTAREA_AVG: double (nullable = true)\n",
      " |-- YEARS_BEGINEXPLUATATION_AVG: double (nullable = true)\n",
      " |-- YEARS_BUILD_AVG: double (nullable = true)\n",
      " |-- COMMONAREA_AVG: double (nullable = true)\n",
      " |-- ELEVATORS_AVG: double (nullable = true)\n",
      " |-- ENTRANCES_AVG: double (nullable = true)\n",
      " |-- FLOORSMAX_AVG: double (nullable = true)\n",
      " |-- FLOORSMIN_AVG: double (nullable = true)\n",
      " |-- LANDAREA_AVG: double (nullable = true)\n",
      " |-- LIVINGAPARTMENTS_AVG: double (nullable = true)\n",
      " |-- LIVINGAREA_AVG: double (nullable = true)\n",
      " |-- NONLIVINGAPARTMENTS_AVG: double (nullable = true)\n",
      " |-- NONLIVINGAREA_AVG: double (nullable = true)\n",
      " |-- APARTMENTS_MODE: double (nullable = true)\n",
      " |-- BASEMENTAREA_MODE: double (nullable = true)\n",
      " |-- YEARS_BEGINEXPLUATATION_MODE: double (nullable = true)\n",
      " |-- YEARS_BUILD_MODE: double (nullable = true)\n",
      " |-- COMMONAREA_MODE: double (nullable = true)\n",
      " |-- ELEVATORS_MODE: double (nullable = true)\n",
      " |-- ENTRANCES_MODE: double (nullable = true)\n",
      " |-- FLOORSMAX_MODE: double (nullable = true)\n",
      " |-- FLOORSMIN_MODE: double (nullable = true)\n",
      " |-- LANDAREA_MODE: double (nullable = true)\n",
      " |-- LIVINGAPARTMENTS_MODE: double (nullable = true)\n",
      " |-- LIVINGAREA_MODE: double (nullable = true)\n",
      " |-- NONLIVINGAPARTMENTS_MODE: double (nullable = true)\n",
      " |-- NONLIVINGAREA_MODE: double (nullable = true)\n",
      " |-- APARTMENTS_MEDI: double (nullable = true)\n",
      " |-- BASEMENTAREA_MEDI: double (nullable = true)\n",
      " |-- YEARS_BEGINEXPLUATATION_MEDI: double (nullable = true)\n",
      " |-- YEARS_BUILD_MEDI: double (nullable = true)\n",
      " |-- COMMONAREA_MEDI: double (nullable = true)\n",
      " |-- ELEVATORS_MEDI: double (nullable = true)\n",
      " |-- ENTRANCES_MEDI: double (nullable = true)\n",
      " |-- FLOORSMAX_MEDI: double (nullable = true)\n",
      " |-- FLOORSMIN_MEDI: double (nullable = true)\n",
      " |-- LANDAREA_MEDI: double (nullable = true)\n",
      " |-- LIVINGAPARTMENTS_MEDI: double (nullable = true)\n",
      " |-- LIVINGAREA_MEDI: double (nullable = true)\n",
      " |-- NONLIVINGAPARTMENTS_MEDI: double (nullable = true)\n",
      " |-- NONLIVINGAREA_MEDI: double (nullable = true)\n",
      " |-- FONDKAPREMONT_MODE: string (nullable = true)\n",
      " |-- HOUSETYPE_MODE: string (nullable = true)\n",
      " |-- TOTALAREA_MODE: double (nullable = true)\n",
      " |-- WALLSMATERIAL_MODE: string (nullable = true)\n",
      " |-- EMERGENCYSTATE_MODE: string (nullable = true)\n",
      " |-- OBS_30_CNT_SOCIAL_CIRCLE: integer (nullable = true)\n",
      " |-- DEF_30_CNT_SOCIAL_CIRCLE: integer (nullable = true)\n",
      " |-- OBS_60_CNT_SOCIAL_CIRCLE: integer (nullable = true)\n",
      " |-- DEF_60_CNT_SOCIAL_CIRCLE: integer (nullable = true)\n",
      " |-- DAYS_LAST_PHONE_CHANGE: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_2: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_3: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_4: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_5: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_6: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_7: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_8: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_9: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_10: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_11: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_12: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_13: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_14: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_15: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_16: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_17: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_18: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_19: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_20: integer (nullable = true)\n",
      " |-- FLAG_DOCUMENT_21: integer (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_HOUR: integer (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_DAY: integer (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_WEEK: integer (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_MON: integer (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_QRT: integer (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_YEAR: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257512"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|SK_ID_CURR|\n",
      "+----------+\n",
      "|    157876|\n",
      "|    157878|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.select(\"SK_ID_CURR\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the data to table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.registerTempTable('train_table')\n",
    "sqlContext =  SQLContext(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|SK_ID_CURR|FLAG_DOCUMENT_21|\n",
      "+----------+----------------+\n",
      "|    157876|               0|\n",
      "|    157878|               0|\n",
      "|    157879|               0|\n",
      "|    157880|               0|\n",
      "|    157881|               0|\n",
      "+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select SK_ID_CURR,FLAG_DOCUMENT_21 from train_table').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[31] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.rdd.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|SK_ID_CURR|\n",
      "+----------+\n",
      "|    157876|\n",
      "|    157878|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.select(\"SK_ID_CURR\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>NAME_TYPE_SUITE</th>\n",
       "      <th>NAME_INCOME_TYPE</th>\n",
       "      <th>NAME_EDUCATION_TYPE</th>\n",
       "      <th>NAME_FAMILY_STATUS</th>\n",
       "      <th>NAME_HOUSING_TYPE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_PUBLISH</th>\n",
       "      <th>OWN_CAR_AGE</th>\n",
       "      <th>FLAG_MOBIL</th>\n",
       "      <th>FLAG_EMP_PHONE</th>\n",
       "      <th>FLAG_WORK_PHONE</th>\n",
       "      <th>FLAG_CONT_MOBILE</th>\n",
       "      <th>FLAG_PHONE</th>\n",
       "      <th>FLAG_EMAIL</th>\n",
       "      <th>OCCUPATION_TYPE</th>\n",
       "      <th>CNT_FAM_MEMBERS</th>\n",
       "      <th>REGION_RATING_CLIENT</th>\n",
       "      <th>REGION_RATING_CLIENT_W_CITY</th>\n",
       "      <th>WEEKDAY_APPR_PROCESS_START</th>\n",
       "      <th>HOUR_APPR_PROCESS_START</th>\n",
       "      <th>REG_REGION_NOT_LIVE_REGION</th>\n",
       "      <th>REG_REGION_NOT_WORK_REGION</th>\n",
       "      <th>LIVE_REGION_NOT_WORK_REGION</th>\n",
       "      <th>REG_CITY_NOT_LIVE_CITY</th>\n",
       "      <th>REG_CITY_NOT_WORK_CITY</th>\n",
       "      <th>LIVE_CITY_NOT_WORK_CITY</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "      <th>EXT_SOURCE_1</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <th>APARTMENTS_AVG</th>\n",
       "      <th>BASEMENTAREA_AVG</th>\n",
       "      <th>YEARS_BEGINEXPLUATATION_AVG</th>\n",
       "      <th>YEARS_BUILD_AVG</th>\n",
       "      <th>COMMONAREA_AVG</th>\n",
       "      <th>ELEVATORS_AVG</th>\n",
       "      <th>ENTRANCES_AVG</th>\n",
       "      <th>FLOORSMAX_AVG</th>\n",
       "      <th>FLOORSMIN_AVG</th>\n",
       "      <th>LANDAREA_AVG</th>\n",
       "      <th>LIVINGAPARTMENTS_AVG</th>\n",
       "      <th>LIVINGAREA_AVG</th>\n",
       "      <th>NONLIVINGAPARTMENTS_AVG</th>\n",
       "      <th>NONLIVINGAREA_AVG</th>\n",
       "      <th>APARTMENTS_MODE</th>\n",
       "      <th>BASEMENTAREA_MODE</th>\n",
       "      <th>YEARS_BEGINEXPLUATATION_MODE</th>\n",
       "      <th>YEARS_BUILD_MODE</th>\n",
       "      <th>COMMONAREA_MODE</th>\n",
       "      <th>ELEVATORS_MODE</th>\n",
       "      <th>ENTRANCES_MODE</th>\n",
       "      <th>FLOORSMAX_MODE</th>\n",
       "      <th>FLOORSMIN_MODE</th>\n",
       "      <th>LANDAREA_MODE</th>\n",
       "      <th>LIVINGAPARTMENTS_MODE</th>\n",
       "      <th>LIVINGAREA_MODE</th>\n",
       "      <th>NONLIVINGAPARTMENTS_MODE</th>\n",
       "      <th>NONLIVINGAREA_MODE</th>\n",
       "      <th>APARTMENTS_MEDI</th>\n",
       "      <th>BASEMENTAREA_MEDI</th>\n",
       "      <th>YEARS_BEGINEXPLUATATION_MEDI</th>\n",
       "      <th>YEARS_BUILD_MEDI</th>\n",
       "      <th>COMMONAREA_MEDI</th>\n",
       "      <th>ELEVATORS_MEDI</th>\n",
       "      <th>ENTRANCES_MEDI</th>\n",
       "      <th>FLOORSMAX_MEDI</th>\n",
       "      <th>FLOORSMIN_MEDI</th>\n",
       "      <th>LANDAREA_MEDI</th>\n",
       "      <th>LIVINGAPARTMENTS_MEDI</th>\n",
       "      <th>LIVINGAREA_MEDI</th>\n",
       "      <th>NONLIVINGAPARTMENTS_MEDI</th>\n",
       "      <th>NONLIVINGAREA_MEDI</th>\n",
       "      <th>FONDKAPREMONT_MODE</th>\n",
       "      <th>HOUSETYPE_MODE</th>\n",
       "      <th>TOTALAREA_MODE</th>\n",
       "      <th>WALLSMATERIAL_MODE</th>\n",
       "      <th>EMERGENCYSTATE_MODE</th>\n",
       "      <th>OBS_30_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DEF_30_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>OBS_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DEF_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DAYS_LAST_PHONE_CHANGE</th>\n",
       "      <th>FLAG_DOCUMENT_2</th>\n",
       "      <th>FLAG_DOCUMENT_3</th>\n",
       "      <th>FLAG_DOCUMENT_4</th>\n",
       "      <th>FLAG_DOCUMENT_5</th>\n",
       "      <th>FLAG_DOCUMENT_6</th>\n",
       "      <th>FLAG_DOCUMENT_7</th>\n",
       "      <th>FLAG_DOCUMENT_8</th>\n",
       "      <th>FLAG_DOCUMENT_9</th>\n",
       "      <th>FLAG_DOCUMENT_10</th>\n",
       "      <th>FLAG_DOCUMENT_11</th>\n",
       "      <th>FLAG_DOCUMENT_12</th>\n",
       "      <th>FLAG_DOCUMENT_13</th>\n",
       "      <th>FLAG_DOCUMENT_14</th>\n",
       "      <th>FLAG_DOCUMENT_15</th>\n",
       "      <th>FLAG_DOCUMENT_16</th>\n",
       "      <th>FLAG_DOCUMENT_17</th>\n",
       "      <th>FLAG_DOCUMENT_18</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>157876</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>343800.0</td>\n",
       "      <td>16155.0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>State servant</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.015221</td>\n",
       "      <td>-19421</td>\n",
       "      <td>-826</td>\n",
       "      <td>-293.0</td>\n",
       "      <td>-2651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Medicine staff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>TUESDAY</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.583251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-180</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>157878</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "      <td>247500.0</td>\n",
       "      <td>945000.0</td>\n",
       "      <td>40167.0</td>\n",
       "      <td>945000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>Pensioner</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.019101</td>\n",
       "      <td>-15322</td>\n",
       "      <td>365243</td>\n",
       "      <td>-7733.0</td>\n",
       "      <td>-4788</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>XNA</td>\n",
       "      <td>0.342195</td>\n",
       "      <td>0.623227</td>\n",
       "      <td>0.622922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1965</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157879</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>540000.0</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>540000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.006207</td>\n",
       "      <td>-11120</td>\n",
       "      <td>-61</td>\n",
       "      <td>-953.0</td>\n",
       "      <td>-3474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sales staff</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>0.076905</td>\n",
       "      <td>0.430050</td>\n",
       "      <td>0.656158</td>\n",
       "      <td>0.0976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>0.0917</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1208</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.1124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0531</td>\n",
       "      <td>Stone, brick</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1585</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157880</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>295168.5</td>\n",
       "      <td>16011.0</td>\n",
       "      <td>238500.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>Working</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.030755</td>\n",
       "      <td>-11824</td>\n",
       "      <td>-4467</td>\n",
       "      <td>-1193.0</td>\n",
       "      <td>-2370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Business Entity Type 2</td>\n",
       "      <td>0.391924</td>\n",
       "      <td>0.447755</td>\n",
       "      <td>0.524496</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.9697</td>\n",
       "      <td>0.5852</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.9697</td>\n",
       "      <td>0.6014</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.9697</td>\n",
       "      <td>0.5907</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>org spec account</td>\n",
       "      <td>block of flats</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1708</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157881</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>298512.0</td>\n",
       "      <td>17266.5</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>Pensioner</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Civil marriage</td>\n",
       "      <td>Municipal apartment</td>\n",
       "      <td>0.018029</td>\n",
       "      <td>-22102</td>\n",
       "      <td>365243</td>\n",
       "      <td>-2536.0</td>\n",
       "      <td>-4052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>XNA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.235905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0      157876       0         Cash loans           F            N   \n",
       "1      157878       0         Cash loans           M            Y   \n",
       "2      157879       0    Revolving loans           M            N   \n",
       "3      157880       0         Cash loans           F            N   \n",
       "4      157881       0         Cash loans           F            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0               Y             0           67500.0    343800.0      16155.0   \n",
       "1               N             2          247500.0    945000.0      40167.0   \n",
       "2               Y             2          180000.0    540000.0      27000.0   \n",
       "3               Y             0          112500.0    295168.5      16011.0   \n",
       "4               Y             0           63000.0    298512.0      17266.5   \n",
       "\n",
       "   AMT_GOODS_PRICE NAME_TYPE_SUITE NAME_INCOME_TYPE  \\\n",
       "0         225000.0   Unaccompanied    State servant   \n",
       "1         945000.0   Unaccompanied        Pensioner   \n",
       "2         540000.0   Unaccompanied          Working   \n",
       "3         238500.0   Unaccompanied          Working   \n",
       "4         270000.0   Unaccompanied        Pensioner   \n",
       "\n",
       "             NAME_EDUCATION_TYPE NAME_FAMILY_STATUS    NAME_HOUSING_TYPE  \\\n",
       "0  Secondary / secondary special            Married    House / apartment   \n",
       "1  Secondary / secondary special            Married    House / apartment   \n",
       "2  Secondary / secondary special            Married    House / apartment   \n",
       "3               Higher education            Married    House / apartment   \n",
       "4  Secondary / secondary special     Civil marriage  Municipal apartment   \n",
       "\n",
       "   REGION_POPULATION_RELATIVE  DAYS_BIRTH  DAYS_EMPLOYED  DAYS_REGISTRATION  \\\n",
       "0                    0.015221      -19421           -826             -293.0   \n",
       "1                    0.019101      -15322         365243            -7733.0   \n",
       "2                    0.006207      -11120            -61             -953.0   \n",
       "3                    0.030755      -11824          -4467            -1193.0   \n",
       "4                    0.018029      -22102         365243            -2536.0   \n",
       "\n",
       "   DAYS_ID_PUBLISH  OWN_CAR_AGE  FLAG_MOBIL  FLAG_EMP_PHONE  FLAG_WORK_PHONE  \\\n",
       "0            -2651          NaN           1               1                1   \n",
       "1            -4788          6.0           1               0                0   \n",
       "2            -3474          NaN           1               1                0   \n",
       "3            -2370          NaN           1               1                1   \n",
       "4            -4052          NaN           1               0                0   \n",
       "\n",
       "   FLAG_CONT_MOBILE  FLAG_PHONE  FLAG_EMAIL OCCUPATION_TYPE  CNT_FAM_MEMBERS  \\\n",
       "0                 1           1           0  Medicine staff              2.0   \n",
       "1                 1           0           0             NaN              4.0   \n",
       "2                 1           1           0     Sales staff              4.0   \n",
       "3                 1           0           0        Laborers              2.0   \n",
       "4                 1           0           0             NaN              2.0   \n",
       "\n",
       "   REGION_RATING_CLIENT  REGION_RATING_CLIENT_W_CITY  \\\n",
       "0                     2                            2   \n",
       "1                     2                            2   \n",
       "2                     2                            2   \n",
       "3                     2                            2   \n",
       "4                     3                            3   \n",
       "\n",
       "  WEEKDAY_APPR_PROCESS_START  HOUR_APPR_PROCESS_START  \\\n",
       "0                    TUESDAY                       14   \n",
       "1                  WEDNESDAY                       16   \n",
       "2                   SATURDAY                       13   \n",
       "3                     SUNDAY                       14   \n",
       "4                   THURSDAY                        4   \n",
       "\n",
       "   REG_REGION_NOT_LIVE_REGION  REG_REGION_NOT_WORK_REGION  \\\n",
       "0                           0                           0   \n",
       "1                           0                           0   \n",
       "2                           0                           1   \n",
       "3                           0                           0   \n",
       "4                           0                           0   \n",
       "\n",
       "   LIVE_REGION_NOT_WORK_REGION  REG_CITY_NOT_LIVE_CITY  \\\n",
       "0                            0                       1   \n",
       "1                            0                       0   \n",
       "2                            1                       0   \n",
       "3                            0                       0   \n",
       "4                            0                       0   \n",
       "\n",
       "   REG_CITY_NOT_WORK_CITY  LIVE_CITY_NOT_WORK_CITY       ORGANIZATION_TYPE  \\\n",
       "0                       1                        0                Medicine   \n",
       "1                       0                        0                     XNA   \n",
       "2                       1                        1  Business Entity Type 3   \n",
       "3                       0                        0  Business Entity Type 2   \n",
       "4                       0                        0                     XNA   \n",
       "\n",
       "   EXT_SOURCE_1  EXT_SOURCE_2  EXT_SOURCE_3  APARTMENTS_AVG  BASEMENTAREA_AVG  \\\n",
       "0           NaN      0.583251           NaN             NaN               NaN   \n",
       "1      0.342195      0.623227      0.622922             NaN               NaN   \n",
       "2      0.076905      0.430050      0.656158          0.0976               NaN   \n",
       "3      0.391924      0.447755      0.524496          0.0155            0.0111   \n",
       "4           NaN      0.235905           NaN             NaN               NaN   \n",
       "\n",
       "   YEARS_BEGINEXPLUATATION_AVG  YEARS_BUILD_AVG  COMMONAREA_AVG  \\\n",
       "0                          NaN              NaN             NaN   \n",
       "1                          NaN              NaN             NaN   \n",
       "2                       0.9861              NaN             NaN   \n",
       "3                       0.9697           0.5852          0.0365   \n",
       "4                          NaN              NaN             NaN   \n",
       "\n",
       "   ELEVATORS_AVG  ENTRANCES_AVG  FLOORSMAX_AVG  FLOORSMIN_AVG  LANDAREA_AVG  \\\n",
       "0            NaN            NaN            NaN            NaN           NaN   \n",
       "1            NaN            NaN            NaN            NaN           NaN   \n",
       "2         0.1064         0.0917         0.3333            NaN        0.0577   \n",
       "3            NaN         0.0862         0.0417         0.0833        0.0285   \n",
       "4            NaN            NaN            NaN            NaN           NaN   \n",
       "\n",
       "   LIVINGAPARTMENTS_AVG  LIVINGAREA_AVG  NONLIVINGAPARTMENTS_AVG  \\\n",
       "0                   NaN             NaN                      NaN   \n",
       "1                   NaN             NaN                      NaN   \n",
       "2                   NaN          0.0964                      NaN   \n",
       "3                0.0126          0.0167                      0.0   \n",
       "4                   NaN             NaN                      NaN   \n",
       "\n",
       "   NONLIVINGAREA_AVG  APARTMENTS_MODE  BASEMENTAREA_MODE  \\\n",
       "0                NaN              NaN                NaN   \n",
       "1                NaN              NaN                NaN   \n",
       "2             0.0304           0.0693                NaN   \n",
       "3             0.0196           0.0126             0.0115   \n",
       "4                NaN              NaN                NaN   \n",
       "\n",
       "   YEARS_BEGINEXPLUATATION_MODE  YEARS_BUILD_MODE  COMMONAREA_MODE  \\\n",
       "0                           NaN               NaN              NaN   \n",
       "1                           NaN               NaN              NaN   \n",
       "2                        0.9851               NaN              NaN   \n",
       "3                        0.9697            0.6014           0.0369   \n",
       "4                           NaN               NaN              NaN   \n",
       "\n",
       "   ELEVATORS_MODE  ENTRANCES_MODE  FLOORSMAX_MODE  FLOORSMIN_MODE  \\\n",
       "0             NaN             NaN             NaN             NaN   \n",
       "1             NaN             NaN             NaN             NaN   \n",
       "2          0.1208          0.1034          0.3333             NaN   \n",
       "3             NaN          0.0690          0.0417          0.0833   \n",
       "4             NaN             NaN             NaN             NaN   \n",
       "\n",
       "   LANDAREA_MODE  LIVINGAPARTMENTS_MODE  LIVINGAREA_MODE  \\\n",
       "0            NaN                    NaN              NaN   \n",
       "1            NaN                    NaN              NaN   \n",
       "2         0.0568                    NaN           0.0704   \n",
       "3         0.0278                  0.011           0.0131   \n",
       "4            NaN                    NaN              NaN   \n",
       "\n",
       "   NONLIVINGAPARTMENTS_MODE  NONLIVINGAREA_MODE  APARTMENTS_MEDI  \\\n",
       "0                       NaN                 NaN              NaN   \n",
       "1                       NaN                 NaN              NaN   \n",
       "2                       NaN              0.0322           0.1124   \n",
       "3                       0.0              0.0174           0.0156   \n",
       "4                       NaN                 NaN              NaN   \n",
       "\n",
       "   BASEMENTAREA_MEDI  YEARS_BEGINEXPLUATATION_MEDI  YEARS_BUILD_MEDI  \\\n",
       "0                NaN                           NaN               NaN   \n",
       "1                NaN                           NaN               NaN   \n",
       "2                NaN                        0.9861               NaN   \n",
       "3             0.0111                        0.9697            0.5907   \n",
       "4                NaN                           NaN               NaN   \n",
       "\n",
       "   COMMONAREA_MEDI  ELEVATORS_MEDI  ENTRANCES_MEDI  FLOORSMAX_MEDI  \\\n",
       "0              NaN             NaN             NaN             NaN   \n",
       "1              NaN             NaN             NaN             NaN   \n",
       "2              NaN            0.12          0.1034          0.3333   \n",
       "3           0.0368             NaN          0.0862          0.0417   \n",
       "4              NaN             NaN             NaN             NaN   \n",
       "\n",
       "   FLOORSMIN_MEDI  LANDAREA_MEDI  LIVINGAPARTMENTS_MEDI  LIVINGAREA_MEDI  \\\n",
       "0             NaN            NaN                    NaN              NaN   \n",
       "1             NaN            NaN                    NaN              NaN   \n",
       "2             NaN         0.0587                    NaN           0.1123   \n",
       "3          0.0833         0.0290                 0.0128           0.0170   \n",
       "4             NaN            NaN                    NaN              NaN   \n",
       "\n",
       "   NONLIVINGAPARTMENTS_MEDI  NONLIVINGAREA_MEDI FONDKAPREMONT_MODE  \\\n",
       "0                       NaN                 NaN                NaN   \n",
       "1                       NaN                 NaN                NaN   \n",
       "2                       NaN               0.031                NaN   \n",
       "3                       0.0               0.020   org spec account   \n",
       "4                       NaN                 NaN                NaN   \n",
       "\n",
       "   HOUSETYPE_MODE  TOTALAREA_MODE WALLSMATERIAL_MODE EMERGENCYSTATE_MODE  \\\n",
       "0             NaN             NaN                NaN                 NaN   \n",
       "1             NaN             NaN                NaN                 NaN   \n",
       "2             NaN          0.0531       Stone, brick                  No   \n",
       "3  block of flats          0.0200              Mixed                  No   \n",
       "4             NaN             NaN                NaN                 NaN   \n",
       "\n",
       "   OBS_30_CNT_SOCIAL_CIRCLE  DEF_30_CNT_SOCIAL_CIRCLE  \\\n",
       "0                       4.0                       1.0   \n",
       "1                       2.0                       2.0   \n",
       "2                       0.0                       0.0   \n",
       "3                       0.0                       0.0   \n",
       "4                       1.0                       0.0   \n",
       "\n",
       "   OBS_60_CNT_SOCIAL_CIRCLE  DEF_60_CNT_SOCIAL_CIRCLE  DAYS_LAST_PHONE_CHANGE  \\\n",
       "0                       4.0                       1.0                    -180   \n",
       "1                       2.0                       2.0                   -1965   \n",
       "2                       0.0                       0.0                   -1585   \n",
       "3                       0.0                       0.0                   -1708   \n",
       "4                       1.0                       0.0                      -1   \n",
       "\n",
       "   FLAG_DOCUMENT_2  FLAG_DOCUMENT_3  FLAG_DOCUMENT_4  FLAG_DOCUMENT_5  \\\n",
       "0                0                1                0                0   \n",
       "1                0                0                0                0   \n",
       "2                0                0                0                0   \n",
       "3                0                1                0                0   \n",
       "4                0                1                0                0   \n",
       "\n",
       "   FLAG_DOCUMENT_6  FLAG_DOCUMENT_7  FLAG_DOCUMENT_8  FLAG_DOCUMENT_9  \\\n",
       "0                0                0                0                0   \n",
       "1                0                0                1                0   \n",
       "2                0                0                0                0   \n",
       "3                0                0                0                0   \n",
       "4                0                0                0                0   \n",
       "\n",
       "   FLAG_DOCUMENT_10  FLAG_DOCUMENT_11  FLAG_DOCUMENT_12  FLAG_DOCUMENT_13  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   FLAG_DOCUMENT_14  FLAG_DOCUMENT_15  FLAG_DOCUMENT_16  FLAG_DOCUMENT_17  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   FLAG_DOCUMENT_18  FLAG_DOCUMENT_19  FLAG_DOCUMENT_20  FLAG_DOCUMENT_21  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_HOUR  AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         0.0                        0.0   \n",
       "4                         NaN                        NaN   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         0.0                        0.0   \n",
       "4                         NaN                        NaN   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "0                        0.0                         1.0  \n",
       "1                        1.0                         2.0  \n",
       "2                        0.0                         3.0  \n",
       "3                        1.0                         1.0  \n",
       "4                        NaN                         NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     OCCUPATION_TYPE|\n",
      "+--------------------+\n",
      "|                null|\n",
      "|            Managers|\n",
      "|            HR staff|\n",
      "|      Medicine staff|\n",
      "|         Accountants|\n",
      "|            Laborers|\n",
      "|      Cleaning staff|\n",
      "|Private service s...|\n",
      "|             Drivers|\n",
      "|         Sales staff|\n",
      "|       Realty agents|\n",
      "|            IT staff|\n",
      "|      Security staff|\n",
      "|         Secretaries|\n",
      "|  Low-skill Laborers|\n",
      "|          Core staff|\n",
      "|       Cooking staff|\n",
      "|High skill tech s...|\n",
      "|Waiters/barmen staff|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select distinct OCCUPATION_TYPE from train_table').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the dimensions of our data frame:\n",
      " (257512, 122)\n",
      "Here's the data types of our columns:\n",
      " SK_ID_CURR                      int64\n",
      "TARGET                          int64\n",
      "NAME_CONTRACT_TYPE             object\n",
      "CODE_GENDER                    object\n",
      "FLAG_OWN_CAR                   object\n",
      "FLAG_OWN_REALTY                object\n",
      "CNT_CHILDREN                    int64\n",
      "AMT_INCOME_TOTAL              float64\n",
      "AMT_CREDIT                    float64\n",
      "AMT_ANNUITY                   float64\n",
      "AMT_GOODS_PRICE               float64\n",
      "NAME_TYPE_SUITE                object\n",
      "NAME_INCOME_TYPE               object\n",
      "NAME_EDUCATION_TYPE            object\n",
      "NAME_FAMILY_STATUS             object\n",
      "NAME_HOUSING_TYPE              object\n",
      "REGION_POPULATION_RELATIVE    float64\n",
      "DAYS_BIRTH                      int64\n",
      "DAYS_EMPLOYED                   int64\n",
      "DAYS_REGISTRATION             float64\n",
      "DAYS_ID_PUBLISH                 int64\n",
      "OWN_CAR_AGE                   float64\n",
      "FLAG_MOBIL                      int64\n",
      "FLAG_EMP_PHONE                  int64\n",
      "FLAG_WORK_PHONE                 int64\n",
      "FLAG_CONT_MOBILE                int64\n",
      "FLAG_PHONE                      int64\n",
      "FLAG_EMAIL                      int64\n",
      "OCCUPATION_TYPE                object\n",
      "CNT_FAM_MEMBERS               float64\n",
      "                               ...   \n",
      "DEF_30_CNT_SOCIAL_CIRCLE      float64\n",
      "OBS_60_CNT_SOCIAL_CIRCLE      float64\n",
      "DEF_60_CNT_SOCIAL_CIRCLE      float64\n",
      "DAYS_LAST_PHONE_CHANGE          int64\n",
      "FLAG_DOCUMENT_2                 int64\n",
      "FLAG_DOCUMENT_3                 int64\n",
      "FLAG_DOCUMENT_4                 int64\n",
      "FLAG_DOCUMENT_5                 int64\n",
      "FLAG_DOCUMENT_6                 int64\n",
      "FLAG_DOCUMENT_7                 int64\n",
      "FLAG_DOCUMENT_8                 int64\n",
      "FLAG_DOCUMENT_9                 int64\n",
      "FLAG_DOCUMENT_10                int64\n",
      "FLAG_DOCUMENT_11                int64\n",
      "FLAG_DOCUMENT_12                int64\n",
      "FLAG_DOCUMENT_13                int64\n",
      "FLAG_DOCUMENT_14                int64\n",
      "FLAG_DOCUMENT_15                int64\n",
      "FLAG_DOCUMENT_16                int64\n",
      "FLAG_DOCUMENT_17                int64\n",
      "FLAG_DOCUMENT_18                int64\n",
      "FLAG_DOCUMENT_19                int64\n",
      "FLAG_DOCUMENT_20                int64\n",
      "FLAG_DOCUMENT_21                int64\n",
      "AMT_REQ_CREDIT_BUREAU_HOUR    float64\n",
      "AMT_REQ_CREDIT_BUREAU_DAY     float64\n",
      "AMT_REQ_CREDIT_BUREAU_WEEK    float64\n",
      "AMT_REQ_CREDIT_BUREAU_MON     float64\n",
      "AMT_REQ_CREDIT_BUREAU_QRT     float64\n",
      "AMT_REQ_CREDIT_BUREAU_YEAR    float64\n",
      "Length: 122, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Here's the dimensions of our data frame:\\n\", \n",
    "     train_data_df.shape)\n",
    "print(\"Here's the data types of our columns:\\n\",\n",
    "     train_data_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_df.apply(lambda x: (x.isnull().values.ravel().sum())  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching only columns where null values are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AMT_ANNUITY                         11\n",
       "AMT_GOODS_PRICE                    240\n",
       "NAME_TYPE_SUITE                   1100\n",
       "OCCUPATION_TYPE                  80737\n",
       "CNT_FAM_MEMBERS                      1\n",
       "EXT_SOURCE_2                       534\n",
       "EXT_SOURCE_3                     51021\n",
       "YEARS_BEGINEXPLUATATION_AVG     125613\n",
       "FLOORSMAX_AVG                   128145\n",
       "YEARS_BEGINEXPLUATATION_MODE    125613\n",
       "FLOORSMAX_MODE                  128145\n",
       "YEARS_BEGINEXPLUATATION_MEDI    125613\n",
       "FLOORSMAX_MEDI                  128145\n",
       "TOTALAREA_MODE                  124283\n",
       "EMERGENCYSTATE_MODE             122057\n",
       "OBS_30_CNT_SOCIAL_CIRCLE           853\n",
       "DEF_30_CNT_SOCIAL_CIRCLE           853\n",
       "OBS_60_CNT_SOCIAL_CIRCLE           853\n",
       "DEF_60_CNT_SOCIAL_CIRCLE           853\n",
       "AMT_REQ_CREDIT_BUREAU_HOUR       34785\n",
       "AMT_REQ_CREDIT_BUREAU_DAY        34785\n",
       "AMT_REQ_CREDIT_BUREAU_WEEK       34785\n",
       "AMT_REQ_CREDIT_BUREAU_MON        34785\n",
       "AMT_REQ_CREDIT_BUREAU_QRT        34785\n",
       "AMT_REQ_CREDIT_BUREAU_YEAR       34785\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_columns=train_data_df.columns[train_data_df.isnull().any()]\n",
    "train_data_df[null_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get the columns having null values greater then any value passed as a parameter in the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_columns(num):\n",
    "    null_columns_gt_257510=train_data_df.columns[train_data_df.isnull().sum()>num]\n",
    "    print(train_data_df[null_columns_gt_257510].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEARS_BUILD_AVG             171249\n",
      "COMMONAREA_AVG              179905\n",
      "FLOORSMIN_AVG               174748\n",
      "LIVINGAPARTMENTS_AVG        175973\n",
      "NONLIVINGAPARTMENTS_AVG     178800\n",
      "YEARS_BUILD_MODE            171249\n",
      "COMMONAREA_MODE             179905\n",
      "FLOORSMIN_MODE              174748\n",
      "LIVINGAPARTMENTS_MODE       175973\n",
      "NONLIVINGAPARTMENTS_MODE    178800\n",
      "YEARS_BUILD_MEDI            171249\n",
      "COMMONAREA_MEDI             179905\n",
      "FLOORSMIN_MEDI              174748\n",
      "LIVINGAPARTMENTS_MEDI       175973\n",
      "NONLIVINGAPARTMENTS_MEDI    178800\n",
      "FONDKAPREMONT_MODE          176104\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "null_columns(170000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get the columns having percentage of null values greater then any value passed as a parameter in the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_columns_percent(num):\n",
    "    null_columns_percentage=train_data_df.columns[train_data_df.isnull().mean().round(4) * 100 > float(num)]\n",
    "    print(train_data_df[null_columns_percentage].isnull().mean().round(4) * 100)\n",
    "    return null_columns_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OWN_CAR_AGE                 66.01\n",
      "EXT_SOURCE_1                56.39\n",
      "APARTMENTS_AVG              50.75\n",
      "BASEMENTAREA_AVG            58.54\n",
      "YEARS_BUILD_AVG             66.50\n",
      "COMMONAREA_AVG              69.86\n",
      "ELEVATORS_AVG               53.29\n",
      "ENTRANCES_AVG               50.34\n",
      "FLOORSMIN_AVG               67.86\n",
      "LANDAREA_AVG                59.36\n",
      "LIVINGAPARTMENTS_AVG        68.34\n",
      "LIVINGAREA_AVG              50.18\n",
      "NONLIVINGAPARTMENTS_AVG     69.43\n",
      "NONLIVINGAREA_AVG           55.19\n",
      "APARTMENTS_MODE             50.75\n",
      "BASEMENTAREA_MODE           58.54\n",
      "YEARS_BUILD_MODE            66.50\n",
      "COMMONAREA_MODE             69.86\n",
      "ELEVATORS_MODE              53.29\n",
      "ENTRANCES_MODE              50.34\n",
      "FLOORSMIN_MODE              67.86\n",
      "LANDAREA_MODE               59.36\n",
      "LIVINGAPARTMENTS_MODE       68.34\n",
      "LIVINGAREA_MODE             50.18\n",
      "NONLIVINGAPARTMENTS_MODE    69.43\n",
      "NONLIVINGAREA_MODE          55.19\n",
      "APARTMENTS_MEDI             50.75\n",
      "BASEMENTAREA_MEDI           58.54\n",
      "YEARS_BUILD_MEDI            66.50\n",
      "COMMONAREA_MEDI             69.86\n",
      "ELEVATORS_MEDI              53.29\n",
      "ENTRANCES_MEDI              50.34\n",
      "FLOORSMIN_MEDI              67.86\n",
      "LANDAREA_MEDI               59.36\n",
      "LIVINGAPARTMENTS_MEDI       68.34\n",
      "LIVINGAREA_MEDI             50.18\n",
      "NONLIVINGAPARTMENTS_MEDI    69.43\n",
      "NONLIVINGAREA_MEDI          55.19\n",
      "FONDKAPREMONT_MODE          68.39\n",
      "HOUSETYPE_MODE              50.18\n",
      "WALLSMATERIAL_MODE          50.83\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "null_col_toBeDropped= null_columns_percent(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the dimensions of our data frame:\n",
      " (257512, 122)\n",
      "Here's the data types of our columns:\n",
      " SK_ID_CURR                      int64\n",
      "TARGET                          int64\n",
      "NAME_CONTRACT_TYPE             object\n",
      "CODE_GENDER                    object\n",
      "FLAG_OWN_CAR                   object\n",
      "FLAG_OWN_REALTY                object\n",
      "CNT_CHILDREN                    int64\n",
      "AMT_INCOME_TOTAL              float64\n",
      "AMT_CREDIT                    float64\n",
      "AMT_ANNUITY                   float64\n",
      "AMT_GOODS_PRICE               float64\n",
      "NAME_TYPE_SUITE                object\n",
      "NAME_INCOME_TYPE               object\n",
      "NAME_EDUCATION_TYPE            object\n",
      "NAME_FAMILY_STATUS             object\n",
      "NAME_HOUSING_TYPE              object\n",
      "REGION_POPULATION_RELATIVE    float64\n",
      "DAYS_BIRTH                      int64\n",
      "DAYS_EMPLOYED                   int64\n",
      "DAYS_REGISTRATION             float64\n",
      "DAYS_ID_PUBLISH                 int64\n",
      "OWN_CAR_AGE                   float64\n",
      "FLAG_MOBIL                      int64\n",
      "FLAG_EMP_PHONE                  int64\n",
      "FLAG_WORK_PHONE                 int64\n",
      "FLAG_CONT_MOBILE                int64\n",
      "FLAG_PHONE                      int64\n",
      "FLAG_EMAIL                      int64\n",
      "OCCUPATION_TYPE                object\n",
      "CNT_FAM_MEMBERS               float64\n",
      "                               ...   \n",
      "DEF_30_CNT_SOCIAL_CIRCLE      float64\n",
      "OBS_60_CNT_SOCIAL_CIRCLE      float64\n",
      "DEF_60_CNT_SOCIAL_CIRCLE      float64\n",
      "DAYS_LAST_PHONE_CHANGE          int64\n",
      "FLAG_DOCUMENT_2                 int64\n",
      "FLAG_DOCUMENT_3                 int64\n",
      "FLAG_DOCUMENT_4                 int64\n",
      "FLAG_DOCUMENT_5                 int64\n",
      "FLAG_DOCUMENT_6                 int64\n",
      "FLAG_DOCUMENT_7                 int64\n",
      "FLAG_DOCUMENT_8                 int64\n",
      "FLAG_DOCUMENT_9                 int64\n",
      "FLAG_DOCUMENT_10                int64\n",
      "FLAG_DOCUMENT_11                int64\n",
      "FLAG_DOCUMENT_12                int64\n",
      "FLAG_DOCUMENT_13                int64\n",
      "FLAG_DOCUMENT_14                int64\n",
      "FLAG_DOCUMENT_15                int64\n",
      "FLAG_DOCUMENT_16                int64\n",
      "FLAG_DOCUMENT_17                int64\n",
      "FLAG_DOCUMENT_18                int64\n",
      "FLAG_DOCUMENT_19                int64\n",
      "FLAG_DOCUMENT_20                int64\n",
      "FLAG_DOCUMENT_21                int64\n",
      "AMT_REQ_CREDIT_BUREAU_HOUR    float64\n",
      "AMT_REQ_CREDIT_BUREAU_DAY     float64\n",
      "AMT_REQ_CREDIT_BUREAU_WEEK    float64\n",
      "AMT_REQ_CREDIT_BUREAU_MON     float64\n",
      "AMT_REQ_CREDIT_BUREAU_QRT     float64\n",
      "AMT_REQ_CREDIT_BUREAU_YEAR    float64\n",
      "Length: 122, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Here's the dimensions of our data frame:\\n\", \n",
    "     train_data_df.shape)\n",
    "print(\"Here's the data types of our columns:\\n\",\n",
    "     train_data_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dx_perc(data_frame, col):\n",
    "    \"\"\"Function used to print class distribution for our data set\"\"\"\n",
    "    try:\n",
    "        # Stores value counts\n",
    "        col_vals = data_frame[col].value_counts()\n",
    "        # Resets index to make index a column in data frame\n",
    "        col_vals = col_vals.reset_index()\n",
    "        # If the number of unique instances in column exceeds 20 print warning\n",
    "        if len(col_vals['index']) > 20:\n",
    "            print('Warning: values in column are more than 20 \\nPlease try a column with lower value counts!')\n",
    "        # Else it calculates/prints percentage for each unique value in column\n",
    "        else:\n",
    "            # Create a function to output the percentage\n",
    "            f = lambda x, y: 100 * (x / sum(y))\n",
    "            for i in range(0, len(col_vals['index'])):\n",
    "                print('{0} accounts for {1:.2f}% of the {2} column'\\\n",
    "                      .format(col_vals['index'][i],\n",
    "                              f(col_vals[col].iloc[i],\n",
    "                                col_vals[col]),\n",
    "                              col))\n",
    "    # try-except block goes here if it can't find the column in data frame\n",
    "    except KeyError as e:\n",
    "        print('{0}: Not found'.format(e))\n",
    "        print('Please choose the right column name!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 accounts for 91.92% of the TARGET column\n",
      "1 accounts for 8.08% of the TARGET column\n"
     ]
    }
   ],
   "source": [
    "print_dx_perc(train_data_df, 'TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  236713|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select count(*)  from train_table where TARGET<1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   20799|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select count(*)  from train_table where TARGET>0').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  257512|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select count(*)  from train_table where SK_ID_CURR is not null ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------------+-----------+------------+---------------+------------+----------------+----------+-----------+---------------+---------------+----------------+-------------------+------------------+-----------------+--------------------------+----------+-------------+-----------------+---------------+---------------+---------------+--------------------+---------------------------+--------------------------+-----------------------+--------------------------+--------------------------+---------------------------+----------------------+----------------------+-----------------------+-----------------+------------+------------+---------------------------+-------------+----------------------------+--------------+----------------------------+--------------+--------------+-------------------+------------------------+------------------------+------------------------+------------------------+----------------------+--------------------------+-------------------------+--------------------------+-------------------------+-------------------------+--------------------------+\n",
      "|SK_ID_CURR|TARGET|NAME_CONTRACT_TYPE|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|AMT_CREDIT|AMT_ANNUITY|AMT_GOODS_PRICE|NAME_TYPE_SUITE|NAME_INCOME_TYPE|NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|REGION_POPULATION_RELATIVE|DAYS_BIRTH|DAYS_EMPLOYED|DAYS_REGISTRATION|DAYS_ID_PUBLISH|OCCUPATION_TYPE|CNT_FAM_MEMBERS|REGION_RATING_CLIENT|REGION_RATING_CLIENT_W_CITY|WEEKDAY_APPR_PROCESS_START|HOUR_APPR_PROCESS_START|REG_REGION_NOT_LIVE_REGION|REG_REGION_NOT_WORK_REGION|LIVE_REGION_NOT_WORK_REGION|REG_CITY_NOT_LIVE_CITY|REG_CITY_NOT_WORK_CITY|LIVE_CITY_NOT_WORK_CITY|ORGANIZATION_TYPE|EXT_SOURCE_2|EXT_SOURCE_3|YEARS_BEGINEXPLUATATION_AVG|FLOORSMAX_AVG|YEARS_BEGINEXPLUATATION_MODE|FLOORSMAX_MODE|YEARS_BEGINEXPLUATATION_MEDI|FLOORSMAX_MEDI|TOTALAREA_MODE|EMERGENCYSTATE_MODE|OBS_30_CNT_SOCIAL_CIRCLE|DEF_30_CNT_SOCIAL_CIRCLE|OBS_60_CNT_SOCIAL_CIRCLE|DEF_60_CNT_SOCIAL_CIRCLE|DAYS_LAST_PHONE_CHANGE|AMT_REQ_CREDIT_BUREAU_HOUR|AMT_REQ_CREDIT_BUREAU_DAY|AMT_REQ_CREDIT_BUREAU_WEEK|AMT_REQ_CREDIT_BUREAU_MON|AMT_REQ_CREDIT_BUREAU_QRT|AMT_REQ_CREDIT_BUREAU_YEAR|\n",
      "+----------+------+------------------+-----------+------------+---------------+------------+----------------+----------+-----------+---------------+---------------+----------------+-------------------+------------------+-----------------+--------------------------+----------+-------------+-----------------+---------------+---------------+---------------+--------------------+---------------------------+--------------------------+-----------------------+--------------------------+--------------------------+---------------------------+----------------------+----------------------+-----------------------+-----------------+------------+------------+---------------------------+-------------+----------------------------+--------------+----------------------------+--------------+--------------+-------------------+------------------------+------------------------+------------------------+------------------------+----------------------+--------------------------+-------------------------+--------------------------+-------------------------+-------------------------+--------------------------+\n",
      "|         0|     0|                 0|          0|           0|              0|           0|               0|         0|          0|              0|              0|               0|                  0|                 0|                0|                         0|         0|            0|                0|              0|              0|              0|                   0|                          0|                         0|                      0|                         0|                         0|                          0|                     0|                     0|                      0|                0|           0|           0|                          0|            0|                           0|             0|                           0|             0|             0|                  0|                       0|                       0|                       0|                       0|                     0|                         0|                        0|                         0|                        0|                        0|                         0|\n",
      "+----------+------+------------------+-----------+------------+---------------+------------+----------------+----------+-----------+---------------+---------------+----------------+-------------------+------------------+-----------------+--------------------------+----------+-------------+-----------------+---------------+---------------+---------------+--------------------+---------------------------+--------------------------+-----------------------+--------------------------+--------------------------+---------------------------+----------------------+----------------------+-----------------------+-----------------+------------+------------+---------------------------+-------------+----------------------------+--------------+----------------------------+--------------+--------------+-------------------+------------------------+------------------------+------------------------+------------------------+----------------------+--------------------------+-------------------------+--------------------------+-------------------------+-------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "train_data.select([count(when(isnan(c), c)).alias(c) for c in train_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OWN_CAR_AGE', 'EXT_SOURCE_1', 'APARTMENTS_AVG', 'BASEMENTAREA_AVG',\n",
       "       'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG',\n",
       "       'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG',\n",
       "       'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG',\n",
       "       'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BUILD_MODE',\n",
       "       'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMIN_MODE',\n",
       "       'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE',\n",
       "       'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI',\n",
       "       'BASEMENTAREA_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI',\n",
       "       'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI',\n",
       "       'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI',\n",
       "       'NONLIVINGAREA_MEDI', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE',\n",
       "       'WALLSMATERIAL_MODE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_col_toBeDropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing null columns with more than 60%null values\n",
    "train_data_df.drop( null_col_toBeDropped, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(*null_col_toBeDropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping some more columns afer analysing that they have least effect on target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_col_toBeDropped=['FLAG_MOBIL','FLAG_EMP_PHONE','FLAG_WORK_PHONE','FLAG_CONT_MOBILE','FLAG_PHONE','FLAG_EMAIL','FLAG_DOCUMENT_2','FLAG_DOCUMENT_3','FLAG_DOCUMENT_4','FLAG_DOCUMENT_5','FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8',  'FLAG_DOCUMENT_9','FLAG_DOCUMENT_10','FLAG_DOCUMENT_11','FLAG_DOCUMENT_12','FLAG_DOCUMENT_13','FLAG_DOCUMENT_14','FLAG_DOCUMENT_15','FLAG_DOCUMENT_16','FLAG_DOCUMENT_17','FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20','FLAG_DOCUMENT_21']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df.drop(more_col_toBeDropped,inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(*more_col_toBeDropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257512, 55)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SK_ID_CURR                        int64\n",
       "TARGET                            int64\n",
       "NAME_CONTRACT_TYPE               object\n",
       "CODE_GENDER                      object\n",
       "FLAG_OWN_CAR                     object\n",
       "FLAG_OWN_REALTY                  object\n",
       "CNT_CHILDREN                      int64\n",
       "AMT_INCOME_TOTAL                float64\n",
       "AMT_CREDIT                      float64\n",
       "AMT_ANNUITY                     float64\n",
       "AMT_GOODS_PRICE                 float64\n",
       "NAME_TYPE_SUITE                  object\n",
       "NAME_INCOME_TYPE                 object\n",
       "NAME_EDUCATION_TYPE              object\n",
       "NAME_FAMILY_STATUS               object\n",
       "NAME_HOUSING_TYPE                object\n",
       "REGION_POPULATION_RELATIVE      float64\n",
       "DAYS_BIRTH                        int64\n",
       "DAYS_EMPLOYED                     int64\n",
       "DAYS_REGISTRATION               float64\n",
       "DAYS_ID_PUBLISH                   int64\n",
       "OCCUPATION_TYPE                  object\n",
       "CNT_FAM_MEMBERS                 float64\n",
       "REGION_RATING_CLIENT              int64\n",
       "REGION_RATING_CLIENT_W_CITY       int64\n",
       "WEEKDAY_APPR_PROCESS_START       object\n",
       "HOUR_APPR_PROCESS_START           int64\n",
       "REG_REGION_NOT_LIVE_REGION        int64\n",
       "REG_REGION_NOT_WORK_REGION        int64\n",
       "LIVE_REGION_NOT_WORK_REGION       int64\n",
       "REG_CITY_NOT_LIVE_CITY            int64\n",
       "REG_CITY_NOT_WORK_CITY            int64\n",
       "LIVE_CITY_NOT_WORK_CITY           int64\n",
       "ORGANIZATION_TYPE                object\n",
       "EXT_SOURCE_2                    float64\n",
       "EXT_SOURCE_3                    float64\n",
       "YEARS_BEGINEXPLUATATION_AVG     float64\n",
       "FLOORSMAX_AVG                   float64\n",
       "YEARS_BEGINEXPLUATATION_MODE    float64\n",
       "FLOORSMAX_MODE                  float64\n",
       "YEARS_BEGINEXPLUATATION_MEDI    float64\n",
       "FLOORSMAX_MEDI                  float64\n",
       "TOTALAREA_MODE                  float64\n",
       "EMERGENCYSTATE_MODE              object\n",
       "OBS_30_CNT_SOCIAL_CIRCLE        float64\n",
       "DEF_30_CNT_SOCIAL_CIRCLE        float64\n",
       "OBS_60_CNT_SOCIAL_CIRCLE        float64\n",
       "DEF_60_CNT_SOCIAL_CIRCLE        float64\n",
       "DAYS_LAST_PHONE_CHANGE            int64\n",
       "AMT_REQ_CREDIT_BUREAU_HOUR      float64\n",
       "AMT_REQ_CREDIT_BUREAU_DAY       float64\n",
       "AMT_REQ_CREDIT_BUREAU_WEEK      float64\n",
       "AMT_REQ_CREDIT_BUREAU_MON       float64\n",
       "AMT_REQ_CREDIT_BUREAU_QRT       float64\n",
       "AMT_REQ_CREDIT_BUREAU_YEAR      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_data_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SK_ID_CURR', 'int'),\n",
       " ('TARGET', 'int'),\n",
       " ('NAME_CONTRACT_TYPE', 'string'),\n",
       " ('CODE_GENDER', 'string'),\n",
       " ('FLAG_OWN_CAR', 'string'),\n",
       " ('FLAG_OWN_REALTY', 'string'),\n",
       " ('CNT_CHILDREN', 'int'),\n",
       " ('AMT_INCOME_TOTAL', 'double'),\n",
       " ('AMT_CREDIT', 'double'),\n",
       " ('AMT_ANNUITY', 'double'),\n",
       " ('AMT_GOODS_PRICE', 'double'),\n",
       " ('NAME_TYPE_SUITE', 'string'),\n",
       " ('NAME_INCOME_TYPE', 'string'),\n",
       " ('NAME_EDUCATION_TYPE', 'string'),\n",
       " ('NAME_FAMILY_STATUS', 'string'),\n",
       " ('NAME_HOUSING_TYPE', 'string'),\n",
       " ('REGION_POPULATION_RELATIVE', 'double'),\n",
       " ('DAYS_BIRTH', 'int'),\n",
       " ('DAYS_EMPLOYED', 'int'),\n",
       " ('DAYS_REGISTRATION', 'double'),\n",
       " ('DAYS_ID_PUBLISH', 'int'),\n",
       " ('OCCUPATION_TYPE', 'string'),\n",
       " ('CNT_FAM_MEMBERS', 'int'),\n",
       " ('REGION_RATING_CLIENT', 'int'),\n",
       " ('REGION_RATING_CLIENT_W_CITY', 'int'),\n",
       " ('WEEKDAY_APPR_PROCESS_START', 'string'),\n",
       " ('HOUR_APPR_PROCESS_START', 'int'),\n",
       " ('REG_REGION_NOT_LIVE_REGION', 'int'),\n",
       " ('REG_REGION_NOT_WORK_REGION', 'int'),\n",
       " ('LIVE_REGION_NOT_WORK_REGION', 'int'),\n",
       " ('REG_CITY_NOT_LIVE_CITY', 'int'),\n",
       " ('REG_CITY_NOT_WORK_CITY', 'int'),\n",
       " ('LIVE_CITY_NOT_WORK_CITY', 'int'),\n",
       " ('ORGANIZATION_TYPE', 'string'),\n",
       " ('EXT_SOURCE_2', 'double'),\n",
       " ('EXT_SOURCE_3', 'double'),\n",
       " ('YEARS_BEGINEXPLUATATION_AVG', 'double'),\n",
       " ('FLOORSMAX_AVG', 'double'),\n",
       " ('YEARS_BEGINEXPLUATATION_MODE', 'double'),\n",
       " ('FLOORSMAX_MODE', 'double'),\n",
       " ('YEARS_BEGINEXPLUATATION_MEDI', 'double'),\n",
       " ('FLOORSMAX_MEDI', 'double'),\n",
       " ('TOTALAREA_MODE', 'double'),\n",
       " ('EMERGENCYSTATE_MODE', 'string'),\n",
       " ('OBS_30_CNT_SOCIAL_CIRCLE', 'int'),\n",
       " ('DEF_30_CNT_SOCIAL_CIRCLE', 'int'),\n",
       " ('OBS_60_CNT_SOCIAL_CIRCLE', 'int'),\n",
       " ('DEF_60_CNT_SOCIAL_CIRCLE', 'int'),\n",
       " ('DAYS_LAST_PHONE_CHANGE', 'int'),\n",
       " ('AMT_REQ_CREDIT_BUREAU_HOUR', 'int'),\n",
       " ('AMT_REQ_CREDIT_BUREAU_DAY', 'int'),\n",
       " ('AMT_REQ_CREDIT_BUREAU_WEEK', 'int'),\n",
       " ('AMT_REQ_CREDIT_BUREAU_MON', 'int'),\n",
       " ('AMT_REQ_CREDIT_BUREAU_QRT', 'int'),\n",
       " ('AMT_REQ_CREDIT_BUREAU_YEAR', 'int')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_col=''\n",
    "double_col=''\n",
    "str_col=''\n",
    "for name,dtype in train_data.dtypes:\n",
    "    if dtype =='int':\n",
    "        int_col = int_col+'\\''+name+'\\','\n",
    "    if dtype == 'double':\n",
    "        double_col = double_col+'\\''+name+'\\','\n",
    "    if dtype == 'string':\n",
    "        str_col = str_col+'\\''+name+'\\','\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'SK_ID_CURR','TARGET','CNT_CHILDREN','DAYS_BIRTH','DAYS_EMPLOYED','DAYS_ID_PUBLISH','CNT_FAM_MEMBERS','REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY','HOUR_APPR_PROCESS_START','REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION','LIVE_REGION_NOT_WORK_REGION','REG_CITY_NOT_LIVE_CITY','REG_CITY_NOT_WORK_CITY','LIVE_CITY_NOT_WORK_CITY','OBS_30_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE','DAYS_LAST_PHONE_CHANGE','AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR',\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','AMT_GOODS_PRICE','REGION_POPULATION_RELATIVE','DAYS_REGISTRATION','EXT_SOURCE_2','EXT_SOURCE_3','YEARS_BEGINEXPLUATATION_AVG','FLOORSMAX_AVG','YEARS_BEGINEXPLUATATION_MODE','FLOORSMAX_MODE','YEARS_BEGINEXPLUATATION_MEDI','FLOORSMAX_MEDI','TOTALAREA_MODE',\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'NAME_CONTRACT_TYPE','CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY','NAME_TYPE_SUITE','NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS','NAME_HOUSING_TYPE','OCCUPATION_TYPE','WEEKDAY_APPR_PROCESS_START','ORGANIZATION_TYPE','EMERGENCYSTATE_MODE',\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(*['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','AMT_GOODS_PRICE','REGION_POPULATION_RELATIVE','DAYS_REGISTRATION','EXT_SOURCE_2','EXT_SOURCE_3','YEARS_BEGINEXPLUATATION_AVG','FLOORSMAX_AVG','YEARS_BEGINEXPLUATATION_MODE','FLOORSMAX_MODE','YEARS_BEGINEXPLUATATION_MEDI','FLOORSMAX_MEDI','TOTALAREA_MODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[SK_ID_CURR: int, TARGET: int, NAME_CONTRACT_TYPE: string, CODE_GENDER: string, FLAG_OWN_CAR: string, FLAG_OWN_REALTY: string, CNT_CHILDREN: int, AMT_INCOME_TOTAL: double, AMT_CREDIT: double, AMT_ANNUITY: double, AMT_GOODS_PRICE: double, NAME_TYPE_SUITE: string, NAME_INCOME_TYPE: string, NAME_EDUCATION_TYPE: string, NAME_FAMILY_STATUS: string, NAME_HOUSING_TYPE: string, REGION_POPULATION_RELATIVE: double, DAYS_BIRTH: int, DAYS_EMPLOYED: int, DAYS_REGISTRATION: double, DAYS_ID_PUBLISH: int, OCCUPATION_TYPE: string, CNT_FAM_MEMBERS: int, REGION_RATING_CLIENT: int, REGION_RATING_CLIENT_W_CITY: int, WEEKDAY_APPR_PROCESS_START: string, HOUR_APPR_PROCESS_START: int, REG_REGION_NOT_LIVE_REGION: int, REG_REGION_NOT_WORK_REGION: int, LIVE_REGION_NOT_WORK_REGION: int, REG_CITY_NOT_LIVE_CITY: int, REG_CITY_NOT_WORK_CITY: int, LIVE_CITY_NOT_WORK_CITY: int, ORGANIZATION_TYPE: string, EXT_SOURCE_2: double, EXT_SOURCE_3: double, YEARS_BEGINEXPLUATATION_AVG: double, FLOORSMAX_AVG: double, YEARS_BEGINEXPLUATATION_MODE: double, FLOORSMAX_MODE: double, YEARS_BEGINEXPLUATATION_MEDI: double, FLOORSMAX_MEDI: double, TOTALAREA_MODE: double, EMERGENCYSTATE_MODE: string, OBS_30_CNT_SOCIAL_CIRCLE: int, DEF_30_CNT_SOCIAL_CIRCLE: int, OBS_60_CNT_SOCIAL_CIRCLE: int, DEF_60_CNT_SOCIAL_CIRCLE: int, DAYS_LAST_PHONE_CHANGE: int, AMT_REQ_CREDIT_BUREAU_HOUR: int, AMT_REQ_CREDIT_BUREAU_DAY: int, AMT_REQ_CREDIT_BUREAU_WEEK: int, AMT_REQ_CREDIT_BUREAU_MON: int, AMT_REQ_CREDIT_BUREAU_QRT: int, AMT_REQ_CREDIT_BUREAU_YEAR: int]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Data for Machine Learning\n",
    "The process includes Category Indexing, One-Hot Encoding and VectorAssembler — a feature transformer that merges multiple columns into a vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "categoricalColumns = ['NAME_CONTRACT_TYPE','CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY','NAME_TYPE_SUITE','NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS','NAME_HOUSING_TYPE','OCCUPATION_TYPE','WEEKDAY_APPR_PROCESS_START','ORGANIZATION_TYPE','EMERGENCYSTATE_MODE']\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "label_stringIdx = StringIndexer(inputCol = 'TARGET', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "numericCols = ['SK_ID_CURR','TARGET','CNT_CHILDREN','DAYS_BIRTH','DAYS_EMPLOYED','DAYS_ID_PUBLISH','CNT_FAM_MEMBERS','REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY','HOUR_APPR_PROCESS_START','REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION','LIVE_REGION_NOT_WORK_REGION','REG_CITY_NOT_LIVE_CITY','REG_CITY_NOT_WORK_CITY','LIVE_CITY_NOT_WORK_CITY','OBS_30_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE','DAYS_LAST_PHONE_CHANGE','AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- SK_ID_CURR: integer (nullable = true)\n",
      " |-- TARGET: integer (nullable = true)\n",
      " |-- NAME_CONTRACT_TYPE: string (nullable = true)\n",
      " |-- CODE_GENDER: string (nullable = true)\n",
      " |-- FLAG_OWN_CAR: string (nullable = true)\n",
      " |-- FLAG_OWN_REALTY: string (nullable = true)\n",
      " |-- CNT_CHILDREN: integer (nullable = true)\n",
      " |-- AMT_INCOME_TOTAL: double (nullable = true)\n",
      " |-- AMT_CREDIT: double (nullable = true)\n",
      " |-- AMT_ANNUITY: double (nullable = true)\n",
      " |-- AMT_GOODS_PRICE: double (nullable = true)\n",
      " |-- NAME_TYPE_SUITE: string (nullable = true)\n",
      " |-- NAME_INCOME_TYPE: string (nullable = true)\n",
      " |-- NAME_EDUCATION_TYPE: string (nullable = true)\n",
      " |-- NAME_FAMILY_STATUS: string (nullable = true)\n",
      " |-- NAME_HOUSING_TYPE: string (nullable = true)\n",
      " |-- REGION_POPULATION_RELATIVE: double (nullable = true)\n",
      " |-- DAYS_BIRTH: integer (nullable = true)\n",
      " |-- DAYS_EMPLOYED: integer (nullable = true)\n",
      " |-- DAYS_REGISTRATION: double (nullable = true)\n",
      " |-- DAYS_ID_PUBLISH: integer (nullable = true)\n",
      " |-- OCCUPATION_TYPE: string (nullable = true)\n",
      " |-- CNT_FAM_MEMBERS: integer (nullable = true)\n",
      " |-- REGION_RATING_CLIENT: integer (nullable = true)\n",
      " |-- REGION_RATING_CLIENT_W_CITY: integer (nullable = true)\n",
      " |-- WEEKDAY_APPR_PROCESS_START: string (nullable = true)\n",
      " |-- HOUR_APPR_PROCESS_START: integer (nullable = true)\n",
      " |-- REG_REGION_NOT_LIVE_REGION: integer (nullable = true)\n",
      " |-- REG_REGION_NOT_WORK_REGION: integer (nullable = true)\n",
      " |-- LIVE_REGION_NOT_WORK_REGION: integer (nullable = true)\n",
      " |-- REG_CITY_NOT_LIVE_CITY: integer (nullable = true)\n",
      " |-- REG_CITY_NOT_WORK_CITY: integer (nullable = true)\n",
      " |-- LIVE_CITY_NOT_WORK_CITY: integer (nullable = true)\n",
      " |-- ORGANIZATION_TYPE: string (nullable = true)\n",
      " |-- EXT_SOURCE_2: double (nullable = true)\n",
      " |-- EXT_SOURCE_3: double (nullable = true)\n",
      " |-- YEARS_BEGINEXPLUATATION_AVG: double (nullable = true)\n",
      " |-- FLOORSMAX_AVG: double (nullable = true)\n",
      " |-- YEARS_BEGINEXPLUATATION_MODE: double (nullable = true)\n",
      " |-- FLOORSMAX_MODE: double (nullable = true)\n",
      " |-- YEARS_BEGINEXPLUATATION_MEDI: double (nullable = true)\n",
      " |-- FLOORSMAX_MEDI: double (nullable = true)\n",
      " |-- TOTALAREA_MODE: double (nullable = true)\n",
      " |-- EMERGENCYSTATE_MODE: string (nullable = true)\n",
      " |-- OBS_30_CNT_SOCIAL_CIRCLE: integer (nullable = true)\n",
      " |-- DEF_30_CNT_SOCIAL_CIRCLE: integer (nullable = true)\n",
      " |-- OBS_60_CNT_SOCIAL_CIRCLE: integer (nullable = true)\n",
      " |-- DEF_60_CNT_SOCIAL_CIRCLE: integer (nullable = true)\n",
      " |-- DAYS_LAST_PHONE_CHANGE: integer (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_HOUR: integer (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_DAY: integer (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_WEEK: integer (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_MON: integer (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_QRT: integer (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_YEAR: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(train_data)\n",
    "train_data = pipelineModel.transform(train_data)\n",
    "selectedCols = ['label', 'features'] + cols\n",
    "train_data = train_data.select(selectedCols)\n",
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4290.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 94.0 failed 1 times, most recent failure: Lost task 1.0 in stage 94.0 (TID 354, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2830)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2829)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2829)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 22 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-f5d44f400ff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2018\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Dataset Count: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Dataset Count: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \"\"\"\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4290.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 94.0 failed 1 times, most recent failure: Lost task 1.0 in stage 94.0 (TID 354, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2830)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2829)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2829)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 22 more\n"
     ]
    }
   ],
   "source": [
    "train, test = train_data.randomSplit([0.7, 0.3],seed=2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4338.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 96.0 failed 1 times, most recent failure: Lost task 1.0 in stage 96.0 (TID 356, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 26 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-e3ca7ad2ee0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'features'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlrModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4338.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 96.0 failed 1 times, most recent failure: Lost task 1.0 in stage 96.0 (TID 356, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 26 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
